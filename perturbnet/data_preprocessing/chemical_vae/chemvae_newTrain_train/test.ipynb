{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hengshi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import hyperparameters\n",
    "import mol_utils as mu\n",
    "import mol_callbacks as mol_cb\n",
    "from keras.callbacks import CSVLogger\n",
    "from models import encoder_model, load_encoder\n",
    "from models import decoder_model, load_decoder\n",
    "from models import varLayer, load_varLayer\n",
    "from models import property_predictor_model, load_property_predictor\n",
    "from models import variational_layers, varLayer\n",
    "from functools import partial\n",
    "from keras.layers import Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_data(params):\n",
    "    # @out : Y_train /Y_test : each is list of datasets.\n",
    "    #        i.e. if reg_tasks only : Y_train_reg = Y_train[0]\n",
    "    #             if logit_tasks only : Y_train_logit = Y_train[0]\n",
    "    #             if both reg and logit_tasks : Y_train_reg = Y_train[0], Y_train_reg = 1\n",
    "    #             if no prop tasks : Y_train = []\n",
    "\n",
    "    MAX_LEN = params['MAX_LEN']\n",
    "\n",
    "    CHARS = yaml.safe_load(open(params['char_file']))\n",
    "    params['NCHARS'] = len(CHARS)\n",
    "    NCHARS = len(CHARS)\n",
    "    CHAR_INDICES = dict((c, i) for i, c in enumerate(CHARS))\n",
    "    #INDICES_CHAR = dict((i, c) for i, c in enumerate(CHARS))\n",
    "\n",
    "    ## Load data for properties\n",
    "    if params['do_prop_pred'] and ('data_file' in params):\n",
    "        if \"data_normalization_out\" in params:\n",
    "            normalize_out = params[\"data_normalization_out\"]\n",
    "        else:\n",
    "            normalize_out = None\n",
    "\n",
    "        ################\n",
    "        if (\"reg_prop_tasks\" in params) and (\"logit_prop_tasks\" in params):\n",
    "            smiles, Y_reg, Y_logit = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN,\n",
    "                    reg_tasks=params['reg_prop_tasks'], logit_tasks=params['logit_prop_tasks'],\n",
    "                    normalize_out = normalize_out)\n",
    "        elif \"logit_prop_tasks\" in params:\n",
    "            smiles, Y_logit = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN,\n",
    "                    logit_tasks=params['logit_prop_tasks'], normalize_out=normalize_out)\n",
    "        elif \"reg_prop_tasks\" in params:\n",
    "            smiles, Y_reg = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN,\n",
    "                    reg_tasks=params['reg_prop_tasks'], normalize_out=normalize_out)\n",
    "        else:\n",
    "            raise ValueError(\"please sepcify logit and/or reg tasks\")\n",
    "\n",
    "    ## Load data if no properties\n",
    "    else:\n",
    "        smiles = mu.load_smiles_and_data_df(params['data_file'], MAX_LEN)\n",
    "\n",
    "    if 'limit_data' in params.keys():\n",
    "        sample_idx = np.random.choice(np.arange(len(smiles)), params['limit_data'], replace=False)\n",
    "        smiles=list(np.array(smiles)[sample_idx])\n",
    "        if params['do_prop_pred'] and ('data_file' in params):\n",
    "            if \"reg_prop_tasks\" in params:\n",
    "                Y_reg =  Y_reg[sample_idx]\n",
    "            if \"logit_prop_tasks\" in params:\n",
    "                Y_logit =  Y_logit[sample_idx]\n",
    "\n",
    "    print('Training set size is', len(smiles))\n",
    "    print('first smiles: \\\"', smiles[0], '\\\"')\n",
    "    print('total chars:', NCHARS)\n",
    "\n",
    "    print('Vectorization...')\n",
    "    X = mu.smiles_to_hot(smiles, MAX_LEN, params[\n",
    "                             'PADDING'], CHAR_INDICES, NCHARS)\n",
    "\n",
    "    print('Total Data size', X.shape[0])\n",
    "    if np.shape(X)[0] % params['batch_size'] != 0:\n",
    "        X = X[:np.shape(X)[0] // params['batch_size']\n",
    "              * params['batch_size']]\n",
    "        if params['do_prop_pred']:\n",
    "            if \"reg_prop_tasks\" in params:\n",
    "                Y_reg = Y_reg[:np.shape(Y_reg)[0] // params['batch_size']\n",
    "                      * params['batch_size']]\n",
    "            if \"logit_prop_tasks\" in params:\n",
    "                Y_logit = Y_logit[:np.shape(Y_logit)[0] // params['batch_size']\n",
    "                      * params['batch_size']]\n",
    "\n",
    "    np.random.seed(params['RAND_SEED'])\n",
    "    rand_idx = np.arange(np.shape(X)[0])\n",
    "    np.random.shuffle(rand_idx)\n",
    "\n",
    "    TRAIN_FRAC = 1 - params['val_split']\n",
    "    num_train = int(X.shape[0] * TRAIN_FRAC)\n",
    "\n",
    "    if num_train % params['batch_size'] != 0:\n",
    "        num_train = num_train // params['batch_size'] * \\\n",
    "            params['batch_size']\n",
    "\n",
    "    train_idx, test_idx = rand_idx[: int(num_train)], rand_idx[int(num_train):]\n",
    "\n",
    "    if 'test_idx_file' in params.keys():\n",
    "        np.save(params['test_idx_file'], test_idx)\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    print('shape of input vector : {}', np.shape(X_train))\n",
    "    print('Training set size is {}, after filtering to max length of {}'.format(\n",
    "        np.shape(X_train), MAX_LEN))\n",
    "\n",
    "    if params['do_prop_pred']:\n",
    "        # !# add Y_train and Y_test here\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        if \"reg_prop_tasks\" in params:\n",
    "            Y_reg_train, Y_reg_test = Y_reg[train_idx], Y_reg[test_idx]\n",
    "            Y_train.append(Y_reg_train)\n",
    "            Y_test.append(Y_reg_test)\n",
    "        if \"logit_prop_tasks\" in params:\n",
    "            Y_logit_train, Y_logit_test = Y_logit[train_idx], Y_logit[test_idx]\n",
    "            Y_train.append(Y_logit_train)\n",
    "            Y_test.append(Y_logit_test)\n",
    "\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "    else:\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "def load_models(params):\n",
    "\n",
    "    def identity(x):\n",
    "        return K.identity(x)\n",
    "\n",
    "    # def K_params with kl_loss_var\n",
    "    kl_loss_var = K.variable(params['kl_loss_weight'])\n",
    "\n",
    "    if params['reload_model'] == True:\n",
    "        encoder = load_encoder(params)\n",
    "        decoder = load_decoder(params)\n",
    "        varlayer = load_varLayer(params)\n",
    "    else:\n",
    "        encoder = encoder_model(params)\n",
    "        decoder = decoder_model(params)\n",
    "        varlayer = varLayer(params)\n",
    "\n",
    "    x_in = encoder.inputs[0]\n",
    "\n",
    "    z_mean, enc_output = encoder(x_in)\n",
    "    #z_samp, z_mean_log_var_output = varlayer([z_mean, enc_output])\n",
    "    z_log_var, z_mean_log_var_output = varlayer([z_mean, enc_output])\n",
    "\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "\n",
    "        epsilon = K.random_normal_variable(shape=(params['batch_size'], params['hidden_dim']),\n",
    "                                           mean=0., scale=1.)\n",
    "        # insert kl loss here\n",
    "\n",
    "        z_rand = z_mean + K.exp(z_log_var / 2) * kl_loss_var * epsilon\n",
    "        return K.in_train_phase(z_rand, z_mean)\n",
    "\n",
    "    z_samp = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "    if params['batchnorm_vae']:\n",
    "        z_samp = BatchNormalization(axis=-1)(z_samp)\n",
    "\n",
    "\n",
    "    #z_samp, z_mean_log_var_output = variational_layers(z_mean, enc_output, kl_loss_var, params)\n",
    "    z_mean_log_var_output = Lambda(identity, name='z_mean_log_var')(z_mean_log_var_output)\n",
    "    # Decoder\n",
    "    if params['do_tgru']:\n",
    "        x_out = decoder([z_samp, x_in])\n",
    "    else:\n",
    "        x_out = decoder(z_samp)\n",
    "\n",
    "    x_out = Lambda(identity, name='x_pred')(x_out)\n",
    "    model_outputs = [x_out, z_mean_log_var_output]\n",
    "\n",
    "    AE_only_model = Model(x_in, model_outputs)\n",
    "\n",
    "    if params['do_prop_pred']:\n",
    "        if params['reload_model'] == True:\n",
    "            property_predictor = load_property_predictor(params)\n",
    "        else:\n",
    "            property_predictor = property_predictor_model(params)\n",
    "\n",
    "        if (('reg_prop_tasks' in params) and (len(params['reg_prop_tasks']) > 0 ) and\n",
    "                ('logit_prop_tasks' in params) and (len(params['logit_prop_tasks']) > 0 )):\n",
    "\n",
    "            reg_prop_pred, logit_prop_pred   = property_predictor(z_mean)\n",
    "            reg_prop_pred = Lambda(identity, name='reg_prop_pred')(reg_prop_pred)\n",
    "            logit_prop_pred = Lambda(identity, name='logit_prop_pred')(logit_prop_pred)\n",
    "            model_outputs.extend([reg_prop_pred,  logit_prop_pred])\n",
    "\n",
    "        # regression only scenario\n",
    "        elif ('reg_prop_tasks' in params) and (len(params['reg_prop_tasks']) > 0 ):\n",
    "            reg_prop_pred = property_predictor(z_mean)\n",
    "            reg_prop_pred = Lambda(identity, name='reg_prop_pred')(reg_prop_pred)\n",
    "            model_outputs.append(reg_prop_pred)\n",
    "\n",
    "        # logit only scenario\n",
    "        elif ('logit_prop_tasks' in params) and (len(params['logit_prop_tasks']) > 0 ):\n",
    "            logit_prop_pred = property_predictor(z_mean)\n",
    "            logit_prop_pred = Lambda(identity, name='logit_prop_pred')(logit_prop_pred)\n",
    "            model_outputs.append(logit_prop_pred)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('no logit tasks or regression tasks specified for property prediction')\n",
    "\n",
    "        # making the models:\n",
    "        AE_PP_model = Model(x_in, model_outputs)\n",
    "        return AE_only_model, AE_PP_model, encoder, decoder, varlayer, property_predictor, kl_loss_var\n",
    "\n",
    "    else:\n",
    "        return AE_only_model, encoder, decoder, varlayer, kl_loss_var\n",
    "\n",
    "\n",
    "def kl_loss(truth_dummy, x_mean_log_var_output):\n",
    "    x_mean, x_log_var = tf.split(x_mean_log_var_output, 2, axis=1)\n",
    "    print('x_mean shape in kl_loss: ', x_mean.get_shape())\n",
    "    kl_loss = - 0.5 * \\\n",
    "        K.mean(1 + x_log_var - K.square(x_mean) -\n",
    "              K.exp(x_log_var), axis=-1)\n",
    "    return kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hyper-parameters:\n",
      "name                      - zinc        \n",
      "MAX_LEN                   - 120         \n",
      "data_file                 - 250k_rndm_zinc_drugs_clean_3.csv\n",
      "char_file                 - zinc.json   \n",
      "encoder_weights_file      - zinc_encoder.h5\n",
      "decoder_weights_file      - zinc_decoder.h5\n",
      "varlayer_weights_file     - zinc_varlayer.h5\n",
      "test_idx_file             - test_idx.npy\n",
      "history_file              - history.csv \n",
      "checkpoint_path           - ./          \n",
      "do_prop_pred              - False       \n",
      "TRAIN_MODEL               - True        \n",
      "ENC_DEC_TEST              - False       \n",
      "PADDING                   - right       \n",
      "RAND_SEED                 - 42          \n",
      "epochs                    - 10          \n",
      "vae_annealer_start        - 29          \n",
      "dropout_rate_mid          - 0.08283292970479479\n",
      "anneal_sigmod_slope       - 0.5106654305791392\n",
      "recurrent_dim             - 488         \n",
      "batch_size                - 126         \n",
      "lr                        - 0.00039192162392520126\n",
      "hidden_dim                - 196         \n",
      "tgru_dropout              - 0.19617749608323892\n",
      "hg_growth_factor          - 1.2281884874932403\n",
      "middle_layer              - 1           \n",
      "momentum                  - 0.9717090063868801\n",
      "rest of parameters are set as default\n",
      "All params: {'reload_model': True, 'prev_epochs': 0, 'batch_size': 126, 'epochs': 10, 'val_split': 0.1, 'loss': 'categorical_crossentropy', 'batchnorm_conv': True, 'conv_activation': 'tanh', 'conv_depth': 4, 'conv_dim_depth': 8, 'conv_dim_width': 8, 'conv_d_growth_factor': 1.15875438383, 'conv_w_growth_factor': 1.1758149644, 'gru_depth': 4, 'rnn_activation': 'tanh', 'recurrent_dim': 488, 'do_tgru': True, 'terminal_GRU_implementation': 0, 'tgru_dropout': 0.19617749608323892, 'temperature': 1.0, 'hg_growth_factor': 1.2281884874932403, 'hidden_dim': 196, 'middle_layer': 1, 'dropout_rate_mid': 0.08283292970479479, 'batchnorm_mid': True, 'activation': 'tanh', 'lr': 0.00039192162392520126, 'momentum': 0.9717090063868801, 'optim': 'adam', 'vae_annealer_start': 29, 'batchnorm_vae': False, 'vae_activation': 'tanh', 'xent_loss_weight': 1.0, 'kl_loss_weight': 1.0, 'anneal_sigmod_slope': 0.5106654305791392, 'freeze_logvar_layer': False, 'freeze_offset': 1, 'do_prop_pred': False, 'prop_pred_depth': 3, 'prop_hidden_dim': 36, 'prop_growth_factor': 0.8, 'prop_pred_activation': 'tanh', 'reg_prop_pred_loss': 'mse', 'logit_prop_pred_loss': 'binary_crossentropy', 'prop_pred_loss_weight': 0.5, 'prop_pred_dropout': 0.0, 'prop_batchnorm': True, 'verbose_print': 0, 'name': 'zinc', 'MAX_LEN': 120, 'data_file': '250k_rndm_zinc_drugs_clean_3.csv', 'char_file': 'zinc.json', 'encoder_weights_file': 'zinc_encoder.h5', 'decoder_weights_file': 'zinc_decoder.h5', 'varlayer_weights_file': 'zinc_varlayer.h5', 'test_idx_file': 'test_idx.npy', 'history_file': 'history.csv', 'checkpoint_path': './', 'TRAIN_MODEL': True, 'ENC_DEC_TEST': False, 'PADDING': 'right', 'RAND_SEED': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "params = hyperparameters.load_params('../models/zinc/exp.json')\n",
    "print(\"All params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['char_file'] = '../models/zinc/zinc.json'\n",
    "params['data_file'] = '../models/zinc/250k_rndm_zinc_drugs_clean_3.csv'\n",
    "params['reload_model'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size is 249455\n",
      "first smiles: \" CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1 \"\n",
      "total chars: 35\n",
      "Vectorization...\n",
      "Total Data size 249455\n",
      "shape of input vector : {} (224406, 120, 35)\n",
      "Training set size is (224406, 120, 35), after filtering to max length of 120\n",
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:432: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:432: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3535: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3535: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:113: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:113: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1210: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1210: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2878: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2878: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /gpfs/accounts/welchjd_root/welchjd/hengshi/GAN/perturb_gan/chemical_vae-master_newTrain/chemvae_newTrain_train/sampled_rnn_tf.py:64: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /gpfs/accounts/welchjd_root/welchjd/hengshi/GAN/perturb_gan/chemical_vae-master_newTrain/chemvae_newTrain_train/sampled_rnn_tf.py:64: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1156: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1156: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:794: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:794: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/optimizers.py:697: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/optimizers.py:697: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2749: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/hengshi/.conda/envs/chemvaeLower/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2749: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mean shape in kl_loss:  (?, 196)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_train, X_test = vectorize_data(params)\n",
    "AE_only_model, encoder, decoder, varlayer, kl_loss_var = load_models(params)\n",
    "\n",
    "# compile models\n",
    "if params['optim'] == 'adam':\n",
    "    optim = Adam(lr=params['lr'], beta_1=params['momentum'])\n",
    "elif params['optim'] == 'rmsprop':\n",
    "    optim = RMSprop(lr=params['lr'], rho=params['momentum'])\n",
    "elif params['optim'] == 'sgd':\n",
    "    optim = SGD(lr=params['lr'], momentum=params['momentum'])\n",
    "else:\n",
    "    raise NotImplemented(\"Please define valid optimizer\")\n",
    "\n",
    "model_losses = {'x_pred': params['loss'],\n",
    "                'z_mean_log_var': kl_loss}\n",
    "\n",
    "# vae metrics, callbacks\n",
    "vae_sig_schedule = partial(mol_cb.sigmoid_schedule, slope=params['anneal_sigmod_slope'],\n",
    "                           start=params['vae_annealer_start'])\n",
    "vae_anneal_callback = mol_cb.WeightAnnealer_epoch(\n",
    "        vae_sig_schedule, kl_loss_var, params['kl_loss_weight'], 'vae' )\n",
    "\n",
    "csv_clb = CSVLogger(params[\"history_file\"], append=False)\n",
    "callbacks = [ vae_anneal_callback, csv_clb]\n",
    "\n",
    "\n",
    "def vae_anneal_metric(y_true, y_pred):\n",
    "    return kl_loss_var\n",
    "\n",
    "xent_loss_weight = K.variable(params['xent_loss_weight'])\n",
    "model_train_targets = {'x_pred':X_train,\n",
    "            'z_mean_log_var':np.ones((np.shape(X_train)[0], params['hidden_dim'] * 2))}\n",
    "model_test_targets = {'x_pred':X_test,\n",
    "    'z_mean_log_var':np.ones((np.shape(X_test)[0], params['hidden_dim'] * 2))}\n",
    "\n",
    "AE_only_model.compile(loss=model_losses,\n",
    "    loss_weights=[xent_loss_weight,\n",
    "      kl_loss_var],\n",
    "    optimizer=optim,\n",
    "    metrics={'x_pred': ['categorical_accuracy',vae_anneal_metric]}\n",
    "    )\n",
    "\n",
    "keras_verbose = params['verbose_print']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'encoder_norm0/keras_learning_phase:0' shape=<unknown> dtype=bool>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = K.in_train_phase(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a._uses_learning_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Print_1:0' shape=<unknown> dtype=bool>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(K.learning_phase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chemvaeLower)",
   "language": "python",
   "name": "chemvaelower"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
